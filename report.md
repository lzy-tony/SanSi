# SanSi

> 三思而后行。

## 简介

### 介绍

三思（SanSi）是一个新型迭代哈希密码，可以接收输入的字符串并将其哈希为 `80` 位的字符串。

不同于传统顺序处理的 sponge 结构，三思在其基础上引入了更多的复杂性：将顺次线性处理分块的结构改变为树形结构，通过在处理顺序的维度引入更多的复杂性（在每次读入时不直接顺序处理而是在积累了一定数量后以树形结构的方式进行处理，即“**三思而后行**”）加强了密码，使得长度扩展攻击等攻击方法更加困难。同时，三思还在 `f` 函数处引入了不同的方法。

本算法由刘方舟、刘子君、刘泽禹、庞元喆、徐明启设计、实现并完成评估。



### 使用方法

#### demo

如果要运行 `demo`，可以在文件夹下运行 `make all` 并运行 `./main <arg1> <arg2>`。其中 `<arg1>` 与 `<arg2>` 为输入输出文件格式。输入文件中以若干行分割若干组需要哈希的字符串。



#### 使用

可以通过在自定义程序中引用 SanSi 算法头文件、并在编译时进行链接进行使用，具体示例如下。

```cpp
#include "sansi.h"

#include <iostream>
#include <cstdio>
#include <cstring>

using namespace std;

int main() {
    Sansi crypto; // 定义
    uint64_t *src;
    
    ...
    crypto.add_block(src); // 添加一个1024位的分块
    cout << crypto.hash() << endl; // 输出哈希结果
    ...
    
    return 0;
}
```



## 设计理念

在哈希函数密码一类问题中，哈希函数 $h$ 需要将无穷域上的输入映射到固定长度上。因此，多数哈希函数密码采用迭代的方式对此进行处理。传统的迭代结构，如 Merkle-Damgard 结构、sponge 结构尽管有所不同，但均采用了顺序执行的方式，即依次处理将数据流嵌入状态中，最终再依据状态进行输出。

在这样的背景下，我们考虑在处理顺序的维度上引入更多的复杂性，通过树形结构而非线性结构处理数据。此时，当我们通过黑盒的视角来观察哈希函数 $h$ 和加密的字符串时，其具体结构会更加不可知，并难以被攻击。

在这一想法的基础上，我们的工作集中于：

- 设计一个合理的计算顺序规则，使得对于不同的字符串输入 $h$ 能够将其以较为混乱的顺序进行处理，并且保证同一个字符串的处理顺序是一定的。
- 将字符串分块的位置信息内嵌入 `f` 函数中，使得攻击者无法通过仅仅调换两个分块的顺序找到碰撞。

我们算法设计的最终结果采取了等待输入积累了一定组数的分组后再进行树形合并处理的方式。在我们看来，这一过程就好比“三思而后行”的过程（积累、再执行），与传统迭代结构中“直接行动”的范式相区别。因此，我们将我们的设计理念融入算法的名称当中，命名其为“三思”。



## 算法组成

### 算法流程

算法的基本流程如下图。

![](./img/flowchart.jpg)



### 迭代结构

sponge 结构的函数主要是由内存状态 $S$、转换函数 $f$ 和填充函数 $P$ 构成的。本算法的树形迭代结构与其有一定的相似之处。在本节中，我们围绕内存状态、树形处理和输出结果三个部分介绍本算法的迭代结构，转换函数 $f$ 和填充函数 $P$ 将在其它章节中进行详细介绍。



#### 内存状态

与采用 sponge 结构的 SHA3 类似，SanSi 采用了大小为 `1600` 比特的内存状态，并将其组织成为一个 $5 \times 5 \times 64$ 的三位张量的形式如下图所示。

![](./img/hash_tensor.jpg)

在 $S$ 中，我们可以通过三维下标 $s_{x, y, z}$ 对某一位进行索引。约定 $s_{x, y}$ 为一个 `64` 位的整数 $s_{x, y, 0} \dots s_{x, y, 63}$。



#### 树形处理

##### 分组内嵌

与传统的线性处理结构不同，SanSi 引入了树形处理结构。约定 `1024` 比特为一个分组，`8` 个分组为一个大组。

对于每个分组输入，算法首先根据组的奇偶性进行判别：奇数位的组将与全0的内存状态相异或；偶数位的组将与全1的内存状态相异或（异或范围为 $S$ 的前 `1024` 比特）。异或后运行一次 `f` 函数进行打乱。



##### 启发值计算

对于每一个输入的块，我们通过其位置和内在信息计算出一个对应的权值作为启发值 $k$。令 $\text{pos}$ 为块在数据流中的位置，则有
$$
k = (\bigoplus \limits_{i = 0}^4 \bigoplus \limits_{j = 0}^4 s_{i, j}) \oplus \text{pos}
$$


##### 合并状态



![](./img/tree_structure.jpg)



#### 输出结果



### `f` 函数

#### 概述



#### $\sigma$ 阶段





#### $\rho$ 阶段



#### $\pi$ 阶段



#### $\alpha$ 阶段



#### $\chi$ 阶段



### 填充

[TODO]



## 算法实现效率

### 测试方法

由于测试目标是算法实现效率，因此在测试时除去了 `IO` 时间与生成数据的时间，并且预先进行了两组数据的插入作为预热。

在具体测试框架中，代码总共处理了 `10000000` 个分块共 `10000000 * 1024` 位的数据，并结合运行时间计算出运行效率，具体代码见 `performance_test.cpp`。

上述流程可在 `make all` 后运行 `./performance_test` 完成。



### 测试结果

测试结果较为稳定，且程序运行效率约为 `534.551 Mbps`，在要求的 `500 Mbps` 以上。



## 评估结果

### 随机性检测

使用[**NIST**统计学工具包](https://csrc.nist.gov/Projects/Random-Bit-Generation/Documentation-and-Software)完成随机性检测。  

通过随机改动字节串中随机选取的部分比特，生成相近的多组明文，经过哈希后，获得待检测文件；之后使用**NIST**工具包对文件进行随机性分析。

上述流程可通过命令：

```bash
bash test.sh
```

完成。共生成 `10240` 组，长度为 `256` 个字符的随机相近明文，经过 SanSi 算法哈希并转换为 ASCII 编码的01串。

将待检测文件分为 `8` 个比特流，对其进行**NIST**工具包的所有随机性检测项目，结果显示，哈希结果通过了所有的随机性检测项目，如下表所示（具体检测结果文件详见 `/result/finalAnalysisReport_test1.txt`）：

|项目|P Value| 通过率 |
|:--:|:--:|:--:|
|Frequency| 0.118170 | 1/1 |
| BlockFrequency | 0.685931 | 1/1 |
|CumulativeSums| 0.236335/0.226213 |  2/2 |
| Rank | 0.346732 | 1/1|
| FFT | 0.977122 | 1/1 |
| Runs | 0.536981| 1/1 |
| LongestRun |0.518900 | 1/1|
|NonOverlappingTemplate| - | 148/148 |
| OverlappingTemplate | 0.673786 | 1/1 |
| ApproximateEntropy | 0.439917 | 1/1 |
| RandomExcursions | - | 8/8 |
| RandomExcursionsVariant | - | 18/18 |
| Serial | 0.955901/0.971477 | 2/2 |
| LinearComplexity | 0.300616 | 1/1 |



### 碰撞攻击检测



## 参考资料
